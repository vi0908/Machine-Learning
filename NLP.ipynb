{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOJk2HT/zheDljEBdVu+xPG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vi0908/Machine-Learning/blob/main/NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## NATURAL LANGUAGE PROCESSING (NLP) IN PYTORCH"
      ],
      "metadata": {
        "id": "liI1lMD7KrAg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from urllib.request import urlretrieve\n",
        "import os \n",
        "if not os.path.isfile('movie-simple.txt'):\n",
        "  text_url = 'https://raw.githubusercontent.com/duke-mlss/Duke-MLSS-2018/master/movie-simple.txt'\n",
        "  urlretrieve(text_url, 'movie-simple.txt')\n"
      ],
      "metadata": {
        "id": "5D-FBW39fhP-"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DOWNLOAD PRE-TRAINED WORD VECTORS (WORD EMBEDDINGS) \n",
        "from urllib.request import urlretrieve\n",
        "import os\n",
        "if not os.path.isfile('mini.h5'):\n",
        "    print(\"Downloading Conceptnet Numberbatch word embeddings\")\n",
        "    conceptnet_url = 'http://conceptnet.s3.amazonaws.com/precomputed-data/2016/numberbatch/17.06/mini.h5'\n",
        "    urlretrieve(conceptnet_url, 'mini.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B5UloO7NK61X",
        "outputId": "eb60f669-0ed5-48ad-b6ec-c1afc537651b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading Conceptnet Numberbatch word embeddings\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# USE THE h5py PACKAGE TO READ THE FILE. \n",
        "import h5py\n",
        "\n",
        "# EXTRACT FROM THE FILE A LIST OF utf-8 ENCODED WORDS\n",
        "with h5py.File('mini.h5', 'r') as f:\n",
        "  all_words = [word.decode('utf-8') for word in f['mat']['axis1'][:]] # List of 362891 strings\n",
        "  all_embeddings = f['mat']['block0_values'][:] # 362891x300 Matrix\n",
        "\n",
        "print(\"all_words dimensions: {} type: {}\".format(len(all_words), type(all_words)))\n",
        "print(\"all_embeddings dimensions: {} type: {}\".format(all_embeddings.shape, type(all_embeddings)))\n",
        "print(\"Random example: \\n word:{}\".format(all_words[1118]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E3uhIEalRMHp",
        "outputId": "5235df82-71fb-4440-ae84-e6a93adabeea"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "all_words dimensions: 362891 type: <class 'list'>\n",
            "all_embeddings dimensions: (362891, 300) type: <class 'numpy.ndarray'>\n",
            "Random example: \n",
            " word:/c/de/armenier\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# RESTRICT OUR VOCABULARY TO JUST THE ENGLISH WORDS\n",
        "english_words = [word[6:] for word in all_words if word.startswith('/c/en/')]\n",
        "english_word_indices = [i for i, word in enumerate(all_words) if word.startswith('/c/en/')]\n",
        "english_embeddings = all_embeddings[english_word_indices]\n",
        "\n",
        "print(\"English words: {}\".format(len(english_words)))\n",
        "print(\"English_embeddings dimension: {}\".format(english_embeddings.shape))\n",
        "print(\"Word: {}\".format(english_words[888]))\n",
        "print(\"Embedding: {}\".format(english_embeddings[888]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xSERK3syVy7I",
        "outputId": "a737c067-1386-42bb-bede-8e9c855c4e9c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "English words: 150875\n",
            "English_embeddings dimension: (150875, 300)\n",
            "Word: accessibility\n",
            "Embedding: [  0   0   1   5  -5   0   1   1   3   6  -1   4  -3  -3  -1   0  -2  -3\n",
            "   3  -7   2  -1  -3  -7  -1   2  -5   1   0  -3   0   0   1   1  -4  -3\n",
            "   3   2   3  -1  -1   4   0   2   3   3  -3  -4   5  -2   0   4   1   5\n",
            "   4  -2   1   0  -8   2   2  -2   1   0  -5   0   2   4   1  -5  -8   1\n",
            "   1   3   0   0   1  10  -6   0  -3   6  -6  -1  -1   1  -2   4  -4  -2\n",
            "  -3  -1  -3   0   2  -2  -4   1   3   5  -3   7   1   0   0   1  -6 -11\n",
            "   9  -7  -1   0   0   0   6   1   3   3   2   1   4  -1  -1   6   0   0\n",
            "  -2  -1  -4  -4   0  -4   1  -3  -1   2  -3   6  -4  -4  -1   5  -4   4\n",
            "   5  -5  -4   1   3   3  -8   0  -2   4  -1  -5   2   0   1  -4  -7   0\n",
            "  -4   3   2   7  -3   0  -2   1  -1  -1   0  -3  -7   3   3  -2  -2  -2\n",
            "   2   7   3  -2   2   9   2   3  -5  -2  -4   1   3  -1  -3  -2  -6   0\n",
            "  -5  -7   0   0   3  -1  -4   1  -6  -4  -5  -6  -1   4   6   2  -1   2\n",
            "  -1   0   2   2  -2   0  -5  -2   1  -5  -2   0  -1   4  -4   0  -1   0\n",
            "  -2  -3   5   2   2   1  -1   4  -3   1  -5  -2   2   0  -1   3   1  -1\n",
            "  -5  -5   3  -3   0   1   0   6   1  -1   0   3  -2   2   0  -3  -4  -2\n",
            "  -1   1   3   0   1  -5   3   0   3  -3  -4   1   5   5   1   4   0   0\n",
            "  -4   1   0   1   6   0   1   0   0   0   3   2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "# Magnitude of a word vector represents frequency of use. \n",
        "# The direction represents its semantics. \n",
        "\n",
        "norms = np.linalg.norm(english_embeddings, axis=1)\n",
        "# As direction is more important than magnitude, we normalize (divide each by its length) our vectors.\n",
        "norm_embeddings = english_embeddings.astype('float32')/norms.astype('float32').reshape((-1,1)) \n",
        "\n",
        "# Dictionary that maps a word to its index in the word embeddings matrix\n",
        "index = {word: i for i, word in enumerate(english_words)}\n",
        "\n"
      ],
      "metadata": {
        "id": "NSisbUWPZUrG"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def similarity_score(w1, w2):\n",
        "  score = np.dot(norm_embeddings[index[w1], :], norm_embeddings[index[w2],: ])\n",
        "  return score\n",
        "w1 = 'dog'\n",
        "w2 = 'sun'\n",
        "print(\"similarity between {} and {}: {}\".format(w1, w2, similarity_score(w1,w2)))\n",
        "print(\"\\n {} norm_embedding: \\n{} \".format(w1, norm_embeddings[index[w1]]))\n",
        "print(\"\\n {} norm_embedding: \\n{} \".format(w2, norm_embeddings[index[w2]]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-g1CI6Rne-Wp",
        "outputId": "de3ebd79-0dff-47ff-88ee-85d60247f764"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "similarity between dog and sun: 0.06799643486738205\n",
            "\n",
            " dog norm_embedding: \n",
            "[ 0.          0.          0.03474716 -0.08686789  0.01737358  0.\n",
            " -0.03474716 -0.06949431  0.01737358 -0.03474716  0.          0.03474716\n",
            "  0.01737358  0.         -0.05212073  0.         -0.05212073 -0.06949431\n",
            "  0.          0.05212073  0.01737358 -0.08686789  0.01737358 -0.05212073\n",
            "  0.          0.08686789 -0.01737358 -0.03474716  0.          0.03474716\n",
            " -0.06949431  0.01737358  0.05212073  0.         -0.05212073  0.03474716\n",
            " -0.05212073  0.08686789 -0.03474716  0.         -0.06949431  0.06949431\n",
            " -0.05212073  0.         -0.01737358 -0.08686789 -0.03474716 -0.10424147\n",
            "  0.05212073 -0.05212073  0.05212073  0.06949431 -0.03474716  0.\n",
            "  0.03474716  0.         -0.03474716  0.05212073  0.08686789  0.\n",
            "  0.10424147  0.01737358 -0.01737358 -0.01737358 -0.08686789 -0.08686789\n",
            "  0.05212073  0.          0.01737358  0.         -0.03474716 -0.06949431\n",
            "  0.06949431  0.         -0.08686789  0.01737358 -0.03474716 -0.08686789\n",
            "  0.01737358  0.          0.05212073  0.          0.03474716  0.\n",
            " -0.05212073  0.08686789 -0.03474716  0.13898863 -0.03474716 -0.05212073\n",
            "  0.03474716  0.         -0.05212073  0.          0.05212073 -0.03474716\n",
            "  0.10424147  0.10424147  0.01737358 -0.08686789  0.03474716 -0.01737358\n",
            "  0.01737358 -0.01737358  0.08686789  0.06949431  0.08686789  0.03474716\n",
            " -0.06949431  0.10424147 -0.01737358  0.          0.         -0.03474716\n",
            " -0.03474716  0.06949431 -0.01737358  0.01737358  0.01737358  0.01737358\n",
            "  0.19110936 -0.03474716  0.03474716  0.          0.01737358 -0.05212073\n",
            " -0.03474716  0.         -0.05212073  0.06949431 -0.05212073 -0.06949431\n",
            "  0.12161505  0.05212073 -0.05212073  0.         -0.10424147 -0.05212073\n",
            " -0.03474716 -0.01737358  0.03474716  0.12161505  0.06949431 -0.08686789\n",
            "  0.03474716  0.05212073 -0.13898863  0.01737358  0.01737358 -0.01737358\n",
            "  0.12161505  0.08686789 -0.01737358 -0.03474716 -0.06949431  0.01737358\n",
            "  0.01737358 -0.08686789 -0.03474716  0.          0.03474716 -0.05212073\n",
            "  0.         -0.03474716 -0.01737358  0.01737358  0.          0.13898863\n",
            "  0.10424147 -0.06949431  0.05212073  0.06949431  0.06949431  0.05212073\n",
            " -0.03474716  0.         -0.01737358 -0.03474716 -0.12161505 -0.01737358\n",
            "  0.01737358  0.          0.05212073 -0.05212073  0.01737358 -0.05212073\n",
            "  0.05212073 -0.08686789 -0.05212073  0.06949431  0.05212073 -0.06949431\n",
            " -0.08686789  0.01737358 -0.10424147  0.03474716 -0.05212073 -0.05212073\n",
            " -0.03474716  0.08686789 -0.03474716  0.         -0.03474716  0.12161505\n",
            "  0.12161505 -0.03474716  0.05212073 -0.05212073  0.         -0.03474716\n",
            " -0.03474716  0.10424147 -0.03474716  0.05212073 -0.01737358  0.06949431\n",
            " -0.06949431  0.         -0.03474716  0.          0.05212073  0.\n",
            "  0.10424147 -0.05212073 -0.05212073 -0.06949431 -0.01737358  0.05212073\n",
            "  0.01737358 -0.08686789  0.12161505 -0.17373578  0.05212073  0.\n",
            "  0.05212073 -0.12161505  0.10424147 -0.01737358  0.03474716  0.08686789\n",
            "  0.06949431 -0.01737358  0.08686789 -0.03474716  0.03474716 -0.01737358\n",
            "  0.         -0.10424147  0.10424147 -0.03474716  0.          0.17373578\n",
            " -0.01737358  0.05212073 -0.03474716  0.          0.          0.\n",
            " -0.05212073  0.          0.13898863  0.01737358  0.01737358  0.03474716\n",
            " -0.03474716  0.         -0.05212073  0.12161505  0.01737358 -0.05212073\n",
            " -0.03474716  0.          0.          0.01737358 -0.01737358 -0.05212073\n",
            "  0.05212073 -0.06949431 -0.03474716 -0.03474716  0.05212073 -0.01737358\n",
            "  0.         -0.01737358 -0.06949431 -0.03474716  0.05212073 -0.05212073\n",
            "  0.01737358 -0.03474716 -0.08686789  0.01737358  0.          0.03474716\n",
            "  0.03474716 -0.06949431  0.01737358  0.05212073  0.03474716 -0.05212073] \n",
            "\n",
            " sun norm_embedding: \n",
            "[ 0.          0.          0.         -0.06957837  0.         -0.01739459\n",
            "  0.01739459  0.         -0.01739459  0.08697297  0.         -0.06957837\n",
            "  0.          0.08697297  0.03478919  0.          0.         -0.01739459\n",
            "  0.03478919  0.         -0.01739459  0.05218378 -0.01739459 -0.13915674\n",
            "  0.06957837  0.01739459  0.06957837 -0.05218378  0.          0.03478919\n",
            "  0.05218378  0.05218378 -0.12176216 -0.05218378  0.20873512  0.\n",
            "  0.          0.01739459  0.05218378  0.01739459  0.          0.\n",
            "  0.01739459  0.          0.10436756 -0.05218378 -0.06957837  0.06957837\n",
            "  0.03478919  0.01739459  0.          0.08697297  0.12176216 -0.08697297\n",
            " -0.01739459 -0.05218378  0.         -0.03478919  0.06957837  0.\n",
            " -0.01739459 -0.01739459 -0.08697297 -0.10436756  0.13915674  0.01739459\n",
            "  0.          0.          0.          0.03478919  0.06957837  0.03478919\n",
            "  0.01739459  0.01739459  0.01739459 -0.06957837  0.06957837 -0.01739459\n",
            " -0.10436756 -0.08697297  0.06957837 -0.05218378  0.          0.\n",
            "  0.         -0.06957837  0.03478919  0.10436756 -0.12176216 -0.10436756\n",
            " -0.05218378 -0.03478919 -0.06957837 -0.01739459  0.01739459 -0.06957837\n",
            "  0.01739459  0.05218378  0.          0.10436756 -0.03478919  0.06957837\n",
            " -0.03478919  0.06957837  0.01739459 -0.03478919  0.06957837 -0.15655135\n",
            "  0.10436756  0.03478919 -0.03478919 -0.08697297  0.01739459 -0.05218378\n",
            " -0.03478919  0.03478919  0.10436756 -0.01739459  0.17394593  0.01739459\n",
            " -0.01739459  0.01739459  0.         -0.01739459 -0.10436756  0.03478919\n",
            "  0.10436756  0.01739459  0.          0.01739459  0.         -0.08697297\n",
            "  0.06957837 -0.03478919  0.          0.         -0.01739459 -0.01739459\n",
            "  0.05218378  0.05218378  0.05218378  0.08697297  0.01739459 -0.05218378\n",
            "  0.          0.          0.08697297  0.03478919  0.05218378  0.\n",
            " -0.03478919  0.01739459  0.06957837 -0.13915674 -0.05218378  0.05218378\n",
            "  0.06957837  0.08697297  0.01739459 -0.01739459  0.10436756 -0.10436756\n",
            " -0.01739459 -0.05218378  0.          0.         -0.13915674  0.\n",
            "  0.03478919  0.          0.          0.03478919  0.15655135 -0.03478919\n",
            " -0.05218378  0.06957837  0.01739459 -0.06957837  0.10436756 -0.05218378\n",
            " -0.06957837 -0.05218378 -0.03478919  0.          0.12176216 -0.01739459\n",
            " -0.10436756  0.          0.05218378  0.06957837  0.08697297  0.\n",
            "  0.         -0.01739459  0.         -0.01739459  0.05218378  0.\n",
            " -0.15655135 -0.05218378  0.03478919 -0.08697297  0.08697297  0.\n",
            "  0.05218378  0.05218378  0.03478919  0.03478919 -0.01739459  0.01739459\n",
            " -0.01739459  0.          0.06957837  0.10436756 -0.08697297 -0.01739459\n",
            "  0.          0.05218378  0.01739459 -0.08697297  0.08697297 -0.03478919\n",
            "  0.         -0.03478919 -0.03478919  0.01739459  0.05218378  0.\n",
            "  0.05218378  0.10436756 -0.03478919  0.          0.01739459  0.05218378\n",
            " -0.03478919 -0.05218378  0.         -0.10436756 -0.05218378  0.\n",
            "  0.03478919  0.12176216  0.         -0.01739459 -0.03478919  0.\n",
            " -0.01739459 -0.05218378  0.05218378  0.03478919  0.          0.10436756\n",
            "  0.01739459  0.         -0.06957837  0.01739459  0.01739459  0.06957837\n",
            " -0.08697297 -0.05218378 -0.01739459  0.01739459 -0.06957837 -0.03478919\n",
            " -0.03478919  0.          0.01739459  0.08697297 -0.05218378 -0.05218378\n",
            " -0.03478919 -0.01739459  0.06957837 -0.05218378  0.          0.01739459\n",
            " -0.03478919 -0.03478919  0.12176216  0.          0.          0.\n",
            " -0.10436756 -0.03478919 -0.03478919  0.          0.03478919  0.01739459\n",
            "  0.03478919  0.03478919 -0.06957837  0.03478919  0.03478919  0.\n",
            " -0.03478919 -0.03478919 -0.03478919  0.05218378  0.          0.        ] \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def closest_to_wordvector(vector, number):\n",
        "  all_scores = np.dot(norm_embeddings, vector)\n",
        "  best_words = list(map(lambda i: english_words[i], reversed(np.argsort(all_scores))))\n",
        "  return best_words[:number]\n",
        "\n",
        "def most_similar(word, number):\n",
        "  return closest_to_wordvector(norm_embeddings[index[word]], number)\n",
        "\n",
        "# Most similar words to a given word:\n",
        "w = 'star'\n",
        "print(\"word: {} \\tmost similar words: {}\".format(w, most_similar(w, 10)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yGft5HfjC0nN",
        "outputId": "ddfa0df4-f805-4fd1-f97c-847f77f2d762"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "word: star \tmost similar words: ['star', 'stars', 'red_supergiant', 'epsilon_aurigae', 'starred', 'circumstellar', 'proxima_centauri', 'red_dwarf', 'starring', 'starlet']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def distant_to_wordvector(vector, number):\n",
        "  all_scores = np.dot(norm_embeddings, vector)\n",
        "  distant_words = list(map(lambda i: english_words[i], np.argsort(all_scores)))\n",
        "  return distant_words[:number]\n",
        "\n",
        "def different_words(word, number):\n",
        "  return(distant_to_wordvector(norm_embeddings[index[word]], number))\n",
        "\n",
        "# Most different words to a given word:\n",
        "w = 'star'\n",
        "print(\"word: {} \\tmost different words: {}\".format(w, different_words(w, 10)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O_qyxTeLGrio",
        "outputId": "13205efb-b779-4897-abed-242110a1ec87"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "word: star \tmost different words: ['asshats', 'agnew', 'exhibitionists', 'brautigan', 'djuna', 'longtail', 'fetishists', 'fariba', 'upshot', 'webpages']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SIMPLE WORD EMBEDDING MODEL (SWEM)\n",
        "Classifying the review of a movie as positive or negative (sentiment analysis)."
      ],
      "metadata": {
        "id": "aIVqWv4-JkTK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "  import string\n",
        "  remove_punct = str.maketrans('','',string.punctuation)\n",
        "\n",
        "  #Function that converts a line into a tuple (x,y), \"x\" is 300-dimensional representation of the words, \"y\" is its label.\n",
        "  def convert_line_to_example(line):\n",
        "    #First character: label (0 negative, 1 positive) \n",
        "    y = int(line[0])\n",
        "\n",
        "    #Split the line into words\n",
        "    words = line[2:].translate(remove_punct).lower().split()\n",
        "\n",
        "    #Embeddings of each word\n",
        "    embeddings = [norm_embeddings[index[w]] for w in words if w in index]\n",
        "\n",
        "    #Mean of the embeddings \n",
        "    x = np.mean(np.vstack(embeddings), axis = 0) # columns\n",
        "    return x, y\n",
        "# Apply the function to each line in the file.\n",
        "xs = []\n",
        "ys = []\n",
        "with open(\"movie-simple.txt\", \"r\", encoding = 'utf-8', errors='ignore') as f:\n",
        "   for line in f:\n",
        "     x,y = convert_line_to_example(line)\n",
        "     xs.append(x)\n",
        "     ys.append(y)\n",
        "# Concatenate all examples into a numpy array\n",
        "xs = np.vstack(xs)\n",
        "ys = np.vstack(ys)"
      ],
      "metadata": {
        "id": "TI0IXy9VJqc2"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Shape of inputs: {}\".format(xs.shape))\n",
        "print(\"Shape of labels: {}\".format(ys.shape))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1evbgb3RhKsS",
        "outputId": "0d14f98a-c7d2-49f1-b673-8781b7a972ae"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of inputs: (1411, 300)\n",
            "Shape of labels: (1411, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_examples = xs.shape[0]\n",
        "\n",
        "print(\"First 20 labels before shuffling: {}\".format(ys[:20,0]))\n",
        "index_shuffle = np.random.permutation(num_examples)\n",
        "xs = xs[index_shuffle,:]\n",
        "ys = ys[index_shuffle,:]\n",
        "print(\"First 20 labels after shuffling: {}\".format(ys[:20, 0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sGs3zmupi53R",
        "outputId": "9f719291-5d53-4227-b71f-0b844476a1cb"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 20 labels before shuffling: [0 1 1 1 1 0 0 0 0 1 1 0 0 0 1 0 0 1 1 1]\n",
            "First 20 labels after shuffling: [0 1 0 1 1 1 0 0 1 0 0 1 0 1 1 1 0 1 1 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "# Floor division. Rounds the result down to the nearest integer\n",
        "num_train = 4*num_examples // 5\n",
        "\n",
        "# Train set\n",
        "x_train = torch.tensor(xs[:num_train])\n",
        "y_train = torch.tensor(ys[:num_train], dtype=torch.float32)\n",
        "\n",
        "# Test set\n",
        "x_test = torch.tensor(xs[num_train:])\n",
        "y_test = torch.tensor(ys[num_train:], dtype=torch.float32)"
      ],
      "metadata": {
        "id": "E1GrFn-ckpc2"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reviews_train = torch.utils.data.TensorDataset(x_train, y_train)\n",
        "reviews_test = torch.utils.data.TensorDataset(x_test, y_test)\n",
        "\n",
        "# DataLoaders\n",
        "train_loader = torch.utils.data.DataLoader(reviews_train, batch_size=100, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(reviews_test, batch_size=100, shuffle=False)"
      ],
      "metadata": {
        "id": "_w4hoXzLrvoM"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from tqdm.notebook import tqdm"
      ],
      "metadata": {
        "id": "ePShGBjFssTD"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SWEM(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.fc1 = nn.Linear(300,64)\n",
        "    # We are only doing binary classification, output will be a single value\n",
        "    self.fc2 = nn.Linear(64,1)\n",
        "  def forward(self, x):\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = self.fc2(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "7lduGcYftAtq"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## TRAINING\n",
        "# Instantiate model\n",
        "model = SWEM()\n",
        "\n",
        "# Loss function -> Binary cross-entropy (BCE) and optimizer -> Adam Optimizer \n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(),lr=0.001)\n",
        "\n",
        "for epoch in range(250):\n",
        "  correct = 0\n",
        "  num_examples = 0\n",
        "  for inputs, labels in tqdm(train_loader):\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Forward pass\n",
        "    y = model(inputs)\n",
        "    loss = criterion(y, labels)\n",
        "\n",
        "    #Backward pass\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    predictions = torch.round(torch.sigmoid(y))\n",
        "    correct += torch.sum((predictions==labels).float())\n",
        "    num_examples += len(inputs)\n",
        "  \n",
        "  # Training progress\n",
        "  if epoch % 25 == 0:\n",
        "    acc = correct / num_examples \n",
        "    print(\"Epoch: {} \\tTrain loss: {} \\tTrain acc: {}\".format(epoch, loss, acc))\n",
        "\n"
      ],
      "metadata": {
        "id": "zXfAr8y3tmr2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Testing\n",
        "correct = 0\n",
        "num_test = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "  for inputs, labels in test_loader:\n",
        "    y = model(inputs)\n",
        "\n",
        "    predictions = torch.round(torch.sigmoid(y))\n",
        "    correct += torch.sum((predictions == labels).float())\n",
        "    num_test += len(inputs)\n",
        "\n",
        "print(\"Test accuracy: {}\".format(correct/num_test)) \n",
        "  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iDjD8TBYtBD9",
        "outputId": "ea96d5b8-6e4a-4a29-e9d4-b5a8a7a51557"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy: 0.9328621625900269\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test some words\n",
        "test_words = [\"men\", \"you\", \"physics\"]\n",
        "\n",
        "for word in test_words:\n",
        "  x = torch.tensor(norm_embeddings[index[word]].reshape(1,300))\n",
        "  y = torch.round(torch.sigmoid(model(x)))\n",
        "  if y == 1:\n",
        "    label = 'positive'\n",
        "  else:\n",
        "    label = 'negative'  \n",
        "  print(\"Sentiment of the word '{}': {}\".format(word, label)) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "94uJWnD-tEW4",
        "outputId": "d4ab45aa-29d3-4cd0-ef98-ffc31431d115"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentiment of the word 'men': negative\n",
            "Sentiment of the word 'you': positive\n",
            "Sentiment of the word 'physics': positive\n"
          ]
        }
      ]
    }
  ]
}