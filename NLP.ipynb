{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN2suw+U8m0NrBZFLGI0lB/"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## NATURAL LANGUAGE PROCESSING (NLP) IN PYTORCH"
      ],
      "metadata": {
        "id": "liI1lMD7KrAg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from urllib.request import urlretrieve\n",
        "import os \n",
        "if not os.path.isfile('movie-simple.txt'):\n",
        "  text_url = 'https://raw.githubusercontent.com/duke-mlss/Duke-MLSS-2018/master/movie-simple.txt'\n",
        "  urlretrieve(text_url, 'movie-simple.txt')\n"
      ],
      "metadata": {
        "id": "5D-FBW39fhP-"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DOWNLOAD PRE-TRAINED WORD VECTORS (WORD EMBEDDINGS) \n",
        "from urllib.request import urlretrieve\n",
        "import os\n",
        "if not os.path.isfile('mini.h5'):\n",
        "    print(\"Downloading Conceptnet Numberbatch word embeddings\")\n",
        "    conceptnet_url = 'http://conceptnet.s3.amazonaws.com/precomputed-data/2016/numberbatch/17.06/mini.h5'\n",
        "    urlretrieve(conceptnet_url, 'mini.h5')"
      ],
      "metadata": {
        "id": "B5UloO7NK61X"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# USE THE h5py PACKAGE TO READ THE FILE. \n",
        "import h5py\n",
        "\n",
        "# EXTRACT FROM THE FILE A LIST OF utf-8 ENCODED WORDS\n",
        "with h5py.File('mini.h5', 'r') as f:\n",
        "  all_words = [word.decode('utf-8') for word in f['mat']['axis1'][:]] # List of 362891 strings\n",
        "  all_embeddings = f['mat']['block0_values'][:] # 362891x300 Matrix\n",
        "\n",
        "print(\"all_words dimensions: {} type: {}\".format(len(all_words), type(all_words)))\n",
        "print(\"all_embeddings dimensions: {} type: {}\".format(all_embeddings.shape, type(all_embeddings)))\n",
        "print(\"Random example: \\n word:{}\".format(all_words[1118]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E3uhIEalRMHp",
        "outputId": "83c86e5c-b896-4228-b26d-798feec3f6ee"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "all_words dimensions: 362891 type: <class 'list'>\n",
            "all_embeddings dimensions: (362891, 300) type: <class 'numpy.ndarray'>\n",
            "Random example: \n",
            " word:/c/de/armenier\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# RESTRICT OUR VOCABULARY TO JUST THE ENGLISH WORDS\n",
        "english_words = [word[6:] for word in all_words if word.startswith('/c/en/')]\n",
        "english_word_indices = [i for i, word in enumerate(all_words) if word.startswith('/c/en/')]\n",
        "english_embeddings = all_embeddings[english_word_indices]\n",
        "\n",
        "print(\"English words: {}\".format(len(english_words)))\n",
        "print(\"English_embeddings dimension: {}\".format(english_embeddings.shape))\n",
        "print(\"Word: {}\".format(english_words[888]))\n",
        "print(\"Embedding: {}\".format(english_embeddings[888]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xSERK3syVy7I",
        "outputId": "0b30bcef-b3ec-47ae-d416-3242d644e93b"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "English words: 150875\n",
            "English_embeddings dimension: (150875, 300)\n",
            "Word: accessibility\n",
            "Embedding: [  0   0   1   5  -5   0   1   1   3   6  -1   4  -3  -3  -1   0  -2  -3\n",
            "   3  -7   2  -1  -3  -7  -1   2  -5   1   0  -3   0   0   1   1  -4  -3\n",
            "   3   2   3  -1  -1   4   0   2   3   3  -3  -4   5  -2   0   4   1   5\n",
            "   4  -2   1   0  -8   2   2  -2   1   0  -5   0   2   4   1  -5  -8   1\n",
            "   1   3   0   0   1  10  -6   0  -3   6  -6  -1  -1   1  -2   4  -4  -2\n",
            "  -3  -1  -3   0   2  -2  -4   1   3   5  -3   7   1   0   0   1  -6 -11\n",
            "   9  -7  -1   0   0   0   6   1   3   3   2   1   4  -1  -1   6   0   0\n",
            "  -2  -1  -4  -4   0  -4   1  -3  -1   2  -3   6  -4  -4  -1   5  -4   4\n",
            "   5  -5  -4   1   3   3  -8   0  -2   4  -1  -5   2   0   1  -4  -7   0\n",
            "  -4   3   2   7  -3   0  -2   1  -1  -1   0  -3  -7   3   3  -2  -2  -2\n",
            "   2   7   3  -2   2   9   2   3  -5  -2  -4   1   3  -1  -3  -2  -6   0\n",
            "  -5  -7   0   0   3  -1  -4   1  -6  -4  -5  -6  -1   4   6   2  -1   2\n",
            "  -1   0   2   2  -2   0  -5  -2   1  -5  -2   0  -1   4  -4   0  -1   0\n",
            "  -2  -3   5   2   2   1  -1   4  -3   1  -5  -2   2   0  -1   3   1  -1\n",
            "  -5  -5   3  -3   0   1   0   6   1  -1   0   3  -2   2   0  -3  -4  -2\n",
            "  -1   1   3   0   1  -5   3   0   3  -3  -4   1   5   5   1   4   0   0\n",
            "  -4   1   0   1   6   0   1   0   0   0   3   2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "# Magnitude of a word vector represents frequency of use. \n",
        "# The direction represents its semantics. \n",
        "\n",
        "norms = np.linalg.norm(english_embeddings, axis=1)\n",
        "# As direction is more important than magnitude, we normalize (divide each by its length) our vectors.\n",
        "norm_embeddings = english_embeddings.astype('float32')/norms.astype('float32').reshape((-1,1)) \n",
        "\n",
        "# Dictionary that maps a word to its index in the word embeddings matrix\n",
        "index = {word: i for i, word in enumerate(english_words)}\n",
        "\n"
      ],
      "metadata": {
        "id": "NSisbUWPZUrG"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def similarity_score(w1, w2):\n",
        "  score = np.dot(norm_embeddings[index[w1], :], norm_embeddings[index[w2],: ])\n",
        "  return score\n",
        "w1 = 'dog'\n",
        "w2 = 'sun'\n",
        "print(\"similarity between {} and {}: {}\".format(w1, w2, similarity_score(w1,w2)))\n",
        "print(\"\\n {} norm_embedding: \\n{} \".format(w1, norm_embeddings[index[w1]]))\n",
        "print(\"\\n {} norm_embedding: \\n{} \".format(w2, norm_embeddings[index[w2]]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-g1CI6Rne-Wp",
        "outputId": "a24cdf98-5de4-469c-8f7e-b88b898249ef"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "similarity between dog and sun: 0.06799643486738205\n",
            "\n",
            " dog norm_embedding: \n",
            "[ 0.          0.          0.03474716 -0.08686789  0.01737358  0.\n",
            " -0.03474716 -0.06949431  0.01737358 -0.03474716  0.          0.03474716\n",
            "  0.01737358  0.         -0.05212073  0.         -0.05212073 -0.06949431\n",
            "  0.          0.05212073  0.01737358 -0.08686789  0.01737358 -0.05212073\n",
            "  0.          0.08686789 -0.01737358 -0.03474716  0.          0.03474716\n",
            " -0.06949431  0.01737358  0.05212073  0.         -0.05212073  0.03474716\n",
            " -0.05212073  0.08686789 -0.03474716  0.         -0.06949431  0.06949431\n",
            " -0.05212073  0.         -0.01737358 -0.08686789 -0.03474716 -0.10424147\n",
            "  0.05212073 -0.05212073  0.05212073  0.06949431 -0.03474716  0.\n",
            "  0.03474716  0.         -0.03474716  0.05212073  0.08686789  0.\n",
            "  0.10424147  0.01737358 -0.01737358 -0.01737358 -0.08686789 -0.08686789\n",
            "  0.05212073  0.          0.01737358  0.         -0.03474716 -0.06949431\n",
            "  0.06949431  0.         -0.08686789  0.01737358 -0.03474716 -0.08686789\n",
            "  0.01737358  0.          0.05212073  0.          0.03474716  0.\n",
            " -0.05212073  0.08686789 -0.03474716  0.13898863 -0.03474716 -0.05212073\n",
            "  0.03474716  0.         -0.05212073  0.          0.05212073 -0.03474716\n",
            "  0.10424147  0.10424147  0.01737358 -0.08686789  0.03474716 -0.01737358\n",
            "  0.01737358 -0.01737358  0.08686789  0.06949431  0.08686789  0.03474716\n",
            " -0.06949431  0.10424147 -0.01737358  0.          0.         -0.03474716\n",
            " -0.03474716  0.06949431 -0.01737358  0.01737358  0.01737358  0.01737358\n",
            "  0.19110936 -0.03474716  0.03474716  0.          0.01737358 -0.05212073\n",
            " -0.03474716  0.         -0.05212073  0.06949431 -0.05212073 -0.06949431\n",
            "  0.12161505  0.05212073 -0.05212073  0.         -0.10424147 -0.05212073\n",
            " -0.03474716 -0.01737358  0.03474716  0.12161505  0.06949431 -0.08686789\n",
            "  0.03474716  0.05212073 -0.13898863  0.01737358  0.01737358 -0.01737358\n",
            "  0.12161505  0.08686789 -0.01737358 -0.03474716 -0.06949431  0.01737358\n",
            "  0.01737358 -0.08686789 -0.03474716  0.          0.03474716 -0.05212073\n",
            "  0.         -0.03474716 -0.01737358  0.01737358  0.          0.13898863\n",
            "  0.10424147 -0.06949431  0.05212073  0.06949431  0.06949431  0.05212073\n",
            " -0.03474716  0.         -0.01737358 -0.03474716 -0.12161505 -0.01737358\n",
            "  0.01737358  0.          0.05212073 -0.05212073  0.01737358 -0.05212073\n",
            "  0.05212073 -0.08686789 -0.05212073  0.06949431  0.05212073 -0.06949431\n",
            " -0.08686789  0.01737358 -0.10424147  0.03474716 -0.05212073 -0.05212073\n",
            " -0.03474716  0.08686789 -0.03474716  0.         -0.03474716  0.12161505\n",
            "  0.12161505 -0.03474716  0.05212073 -0.05212073  0.         -0.03474716\n",
            " -0.03474716  0.10424147 -0.03474716  0.05212073 -0.01737358  0.06949431\n",
            " -0.06949431  0.         -0.03474716  0.          0.05212073  0.\n",
            "  0.10424147 -0.05212073 -0.05212073 -0.06949431 -0.01737358  0.05212073\n",
            "  0.01737358 -0.08686789  0.12161505 -0.17373578  0.05212073  0.\n",
            "  0.05212073 -0.12161505  0.10424147 -0.01737358  0.03474716  0.08686789\n",
            "  0.06949431 -0.01737358  0.08686789 -0.03474716  0.03474716 -0.01737358\n",
            "  0.         -0.10424147  0.10424147 -0.03474716  0.          0.17373578\n",
            " -0.01737358  0.05212073 -0.03474716  0.          0.          0.\n",
            " -0.05212073  0.          0.13898863  0.01737358  0.01737358  0.03474716\n",
            " -0.03474716  0.         -0.05212073  0.12161505  0.01737358 -0.05212073\n",
            " -0.03474716  0.          0.          0.01737358 -0.01737358 -0.05212073\n",
            "  0.05212073 -0.06949431 -0.03474716 -0.03474716  0.05212073 -0.01737358\n",
            "  0.         -0.01737358 -0.06949431 -0.03474716  0.05212073 -0.05212073\n",
            "  0.01737358 -0.03474716 -0.08686789  0.01737358  0.          0.03474716\n",
            "  0.03474716 -0.06949431  0.01737358  0.05212073  0.03474716 -0.05212073] \n",
            "\n",
            " sun norm_embedding: \n",
            "[ 0.          0.          0.         -0.06957837  0.         -0.01739459\n",
            "  0.01739459  0.         -0.01739459  0.08697297  0.         -0.06957837\n",
            "  0.          0.08697297  0.03478919  0.          0.         -0.01739459\n",
            "  0.03478919  0.         -0.01739459  0.05218378 -0.01739459 -0.13915674\n",
            "  0.06957837  0.01739459  0.06957837 -0.05218378  0.          0.03478919\n",
            "  0.05218378  0.05218378 -0.12176216 -0.05218378  0.20873512  0.\n",
            "  0.          0.01739459  0.05218378  0.01739459  0.          0.\n",
            "  0.01739459  0.          0.10436756 -0.05218378 -0.06957837  0.06957837\n",
            "  0.03478919  0.01739459  0.          0.08697297  0.12176216 -0.08697297\n",
            " -0.01739459 -0.05218378  0.         -0.03478919  0.06957837  0.\n",
            " -0.01739459 -0.01739459 -0.08697297 -0.10436756  0.13915674  0.01739459\n",
            "  0.          0.          0.          0.03478919  0.06957837  0.03478919\n",
            "  0.01739459  0.01739459  0.01739459 -0.06957837  0.06957837 -0.01739459\n",
            " -0.10436756 -0.08697297  0.06957837 -0.05218378  0.          0.\n",
            "  0.         -0.06957837  0.03478919  0.10436756 -0.12176216 -0.10436756\n",
            " -0.05218378 -0.03478919 -0.06957837 -0.01739459  0.01739459 -0.06957837\n",
            "  0.01739459  0.05218378  0.          0.10436756 -0.03478919  0.06957837\n",
            " -0.03478919  0.06957837  0.01739459 -0.03478919  0.06957837 -0.15655135\n",
            "  0.10436756  0.03478919 -0.03478919 -0.08697297  0.01739459 -0.05218378\n",
            " -0.03478919  0.03478919  0.10436756 -0.01739459  0.17394593  0.01739459\n",
            " -0.01739459  0.01739459  0.         -0.01739459 -0.10436756  0.03478919\n",
            "  0.10436756  0.01739459  0.          0.01739459  0.         -0.08697297\n",
            "  0.06957837 -0.03478919  0.          0.         -0.01739459 -0.01739459\n",
            "  0.05218378  0.05218378  0.05218378  0.08697297  0.01739459 -0.05218378\n",
            "  0.          0.          0.08697297  0.03478919  0.05218378  0.\n",
            " -0.03478919  0.01739459  0.06957837 -0.13915674 -0.05218378  0.05218378\n",
            "  0.06957837  0.08697297  0.01739459 -0.01739459  0.10436756 -0.10436756\n",
            " -0.01739459 -0.05218378  0.          0.         -0.13915674  0.\n",
            "  0.03478919  0.          0.          0.03478919  0.15655135 -0.03478919\n",
            " -0.05218378  0.06957837  0.01739459 -0.06957837  0.10436756 -0.05218378\n",
            " -0.06957837 -0.05218378 -0.03478919  0.          0.12176216 -0.01739459\n",
            " -0.10436756  0.          0.05218378  0.06957837  0.08697297  0.\n",
            "  0.         -0.01739459  0.         -0.01739459  0.05218378  0.\n",
            " -0.15655135 -0.05218378  0.03478919 -0.08697297  0.08697297  0.\n",
            "  0.05218378  0.05218378  0.03478919  0.03478919 -0.01739459  0.01739459\n",
            " -0.01739459  0.          0.06957837  0.10436756 -0.08697297 -0.01739459\n",
            "  0.          0.05218378  0.01739459 -0.08697297  0.08697297 -0.03478919\n",
            "  0.         -0.03478919 -0.03478919  0.01739459  0.05218378  0.\n",
            "  0.05218378  0.10436756 -0.03478919  0.          0.01739459  0.05218378\n",
            " -0.03478919 -0.05218378  0.         -0.10436756 -0.05218378  0.\n",
            "  0.03478919  0.12176216  0.         -0.01739459 -0.03478919  0.\n",
            " -0.01739459 -0.05218378  0.05218378  0.03478919  0.          0.10436756\n",
            "  0.01739459  0.         -0.06957837  0.01739459  0.01739459  0.06957837\n",
            " -0.08697297 -0.05218378 -0.01739459  0.01739459 -0.06957837 -0.03478919\n",
            " -0.03478919  0.          0.01739459  0.08697297 -0.05218378 -0.05218378\n",
            " -0.03478919 -0.01739459  0.06957837 -0.05218378  0.          0.01739459\n",
            " -0.03478919 -0.03478919  0.12176216  0.          0.          0.\n",
            " -0.10436756 -0.03478919 -0.03478919  0.          0.03478919  0.01739459\n",
            "  0.03478919  0.03478919 -0.06957837  0.03478919  0.03478919  0.\n",
            " -0.03478919 -0.03478919 -0.03478919  0.05218378  0.          0.        ] \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def closest_to_wordvector(vector, number):\n",
        "  all_scores = np.dot(norm_embeddings, vector)\n",
        "  best_words = list(map(lambda i: english_words[i], reversed(np.argsort(all_scores))))\n",
        "  return best_words[:number]\n",
        "\n",
        "def most_similar(word, number):\n",
        "  return closest_to_wordvector(norm_embeddings[index[word]], number)\n",
        "\n",
        "# Most similar words to a given word:\n",
        "w = 'star'\n",
        "print(\"word: {} \\tmost similar words: {}\".format(w, most_similar(w, 10)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yGft5HfjC0nN",
        "outputId": "5a6fc16f-94b8-44b8-b40f-1bbddf4900bb"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "word: star \tmost similar words: ['star', 'stars', 'red_supergiant', 'epsilon_aurigae', 'starred', 'circumstellar', 'proxima_centauri', 'red_dwarf', 'starring', 'starlet']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def distant_to_wordvector(vector, number):\n",
        "  all_scores = np.dot(norm_embeddings, vector)\n",
        "  distant_words = list(map(lambda i: english_words[i], np.argsort(all_scores)))\n",
        "  return distant_words[:number]\n",
        "\n",
        "def different_words(word, number):\n",
        "  return(distant_to_wordvector(norm_embeddings[index[word]], number))\n",
        "\n",
        "# Most different words to a given word:\n",
        "w = 'star'\n",
        "print(\"word: {} \\tmost different words: {}\".format(w, different_words(w, 10)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O_qyxTeLGrio",
        "outputId": "9064a83c-05d2-49d3-c078-a18efec24026"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "word: star \tmost different words: ['asshats', 'agnew', 'exhibitionists', 'brautigan', 'djuna', 'longtail', 'fetishists', 'fariba', 'upshot', 'webpages']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SIMPLE WORD EMBEDDING MODEL (SWEM)\n",
        "Classifying the review of a movie as positive or negative (sentiment analysis)."
      ],
      "metadata": {
        "id": "aIVqWv4-JkTK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "  import string\n",
        "  remove_punct = str.maketrans('','',string.punctuation)\n",
        "\n",
        "  #Function that converts a line into a tuple (x,y), \"x\" is 300-dimensional representation of the words, \"y\" is its label.\n",
        "  def convert_line_to_example(line):\n",
        "    #First character: label (0 negative, 1 positive) \n",
        "    y = int(line[0])\n",
        "\n",
        "    #Split the line into words\n",
        "    words = line[2:].translate(remove_punct).lower().split()\n",
        "\n",
        "    #Embeddings of each word\n",
        "    embeddings = [norm_embeddings[index[w]] for w in words if w in index]\n",
        "\n",
        "    #Mean of the embeddings \n",
        "    x = np.mean(np.vstack(embeddings), axis = 0) # columns\n",
        "    return x, y\n",
        "# Apply the function to each line in the file.\n",
        "xs = []\n",
        "ys = []\n",
        "with open(\"movie-simple.txt\", \"r\", encoding = 'utf-8', errors='ignore') as f:\n",
        "   for line in f:\n",
        "     x,y = convert_line_to_example(line)\n",
        "     xs.append(x)\n",
        "     ys.append(y)\n",
        "# Concatenate all examples into a numpy array\n",
        "xs = np.vstack(xs)\n",
        "ys = np.vstack(ys)"
      ],
      "metadata": {
        "id": "TI0IXy9VJqc2"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Shape of inputs: {}\".format(xs.shape))\n",
        "print(\"Shape of labels: {}\".format(ys.shape))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1evbgb3RhKsS",
        "outputId": "0d14f98a-c7d2-49f1-b673-8781b7a972ae"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of inputs: (1411, 300)\n",
            "Shape of labels: (1411, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_examples = xs.shape[0]\n",
        "\n",
        "print(\"First 20 labels before shuffling: {}\".format(ys[:20,0]))\n",
        "index_shuffle = np.random.permutation(num_examples)\n",
        "xs = xs[index_shuffle,:]\n",
        "ys = ys[index_shuffle,:]\n",
        "print(\"First 20 labels after shuffling: {}\".format(ys[:20, 0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sGs3zmupi53R",
        "outputId": "9f719291-5d53-4227-b71f-0b844476a1cb"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 20 labels before shuffling: [0 1 1 1 1 0 0 0 0 1 1 0 0 0 1 0 0 1 1 1]\n",
            "First 20 labels after shuffling: [0 1 0 1 1 1 0 0 1 0 0 1 0 1 1 1 0 1 1 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "# Floor division. Rounds the result down to the nearest integer\n",
        "num_train = 4*num_examples // 5\n",
        "\n",
        "# Train set\n",
        "x_train = torch.tensor(xs[:num_train])\n",
        "y_train = torch.tensor(ys[:num_train], dtype=torch.float32)\n",
        "\n",
        "# Test set\n",
        "x_test = torch.tensor(xs[num_train:])\n",
        "y_test = torch.tensor(ys[num_train:], dtype=torch.float32)"
      ],
      "metadata": {
        "id": "E1GrFn-ckpc2"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reviews_train = torch.utils.data.TensorDataset(x_train, y_train)\n",
        "reviews_test = torch.utils.data.TensorDataset(x_test, y_test)\n",
        "\n",
        "# DataLoaders\n",
        "train_loader = torch.utils.data.DataLoader(reviews_train, batch_size=100, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(reviews_test, batch_size=100, shuffle=False)"
      ],
      "metadata": {
        "id": "_w4hoXzLrvoM"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from tqdm.notebook import tqdm"
      ],
      "metadata": {
        "id": "ePShGBjFssTD"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SWEM(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.fc1 = nn.Linear(300,64)\n",
        "    # We are only doing binary classification, output will be a single value\n",
        "    self.fc2 = nn.Linear(64,1)\n",
        "  def forward(self, x):\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = self.fc2(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "7lduGcYftAtq"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## TRAINING\n",
        "# Instantiate model\n",
        "model = SWEM()\n",
        "\n",
        "# Loss function -> Binary cross-entropy (BCE) and optimizer -> Adam Optimizer \n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(),lr=0.001)\n",
        "\n",
        "for epoch in range(250):\n",
        "  correct = 0\n",
        "  num_examples = 0\n",
        "  for inputs, labels in tqdm(train_loader):\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Forward pass\n",
        "    y = model(inputs)\n",
        "    loss = criterion(y, labels)\n",
        "\n",
        "    #Backward pass\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    predictions = torch.round(torch.sigmoid(y))\n",
        "    correct += torch.sum((predictions==labels).float())\n",
        "    num_examples += len(inputs)\n",
        "  \n",
        "  # Training progress\n",
        "  if epoch % 25 == 0:\n",
        "    acc = correct / num_examples \n",
        "    print(\"Epoch: {} \\tTrain loss: {} \\tTrain acc: {}\".format(epoch, loss, acc))\n",
        "\n"
      ],
      "metadata": {
        "id": "zXfAr8y3tmr2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Testing\n",
        "correct = 0\n",
        "num_test = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "  for inputs, labels in test_loader:\n",
        "    y = model(inputs)\n",
        "\n",
        "    predictions = torch.round(torch.sigmoid(y))\n",
        "    correct += torch.sum((predictions == labels).float())\n",
        "    num_test += len(inputs)\n",
        "\n",
        "print(\"Test accuracy: {}\".format(correct/num_test)) \n",
        "  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iDjD8TBYtBD9",
        "outputId": "ea96d5b8-6e4a-4a29-e9d4-b5a8a7a51557"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy: 0.9328621625900269\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test some words\n",
        "test_words = [\"men\", \"you\", \"physics\"]\n",
        "\n",
        "for word in test_words:\n",
        "  x = torch.tensor(norm_embeddings[index[word]].reshape(1,300))\n",
        "  y = torch.round(torch.sigmoid(model(x)))\n",
        "  if y == 1:\n",
        "    label = 'positive'\n",
        "  else:\n",
        "    label = 'negative'  \n",
        "  print(\"Sentiment of the word '{}': {}\".format(word, label)) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "94uJWnD-tEW4",
        "outputId": "cd2d8f15-7569-4daf-9b79-37ac8a68083d"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentiment of the word 'men': negative\n",
            "Sentiment of the word 'you': positive\n",
            "Sentiment of the word 'physics': positive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_phrase = \"It is a truth universally acknowledged, that a single man in possession of a good fortune, must be in want of a wife.\"\n",
        "words = test_phrase.translate(remove_punct).lower().split()\n",
        "sum = 0\n",
        "total = 0\n",
        "\n",
        "for word in words:\n",
        "   total+= 1\n",
        "   if word in index:\n",
        "    x = torch.tensor(norm_embeddings[index[word]].reshape(1,300))\n",
        "    y = torch.sigmoid(model(x))\n",
        "    sum += y[0]\n",
        "  \n",
        "\n",
        "mean = sum / total\n",
        "\n",
        "if mean >= 0.5:\n",
        "  print(\"'{}' express something positive.\".format(test_phrase))\n",
        "else:\n",
        "  print(\"'{}' express something negative.\".format(test_phrase))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G1jfVtfgKJjz",
        "outputId": "34e92f8c-60e7-45db-ad65-605322e74939"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'It is a truth universally acknowledged, that a single man in possession of a good fortune, must be in want of a wife.' express something positive.\n"
          ]
        }
      ]
    }
  ]
}